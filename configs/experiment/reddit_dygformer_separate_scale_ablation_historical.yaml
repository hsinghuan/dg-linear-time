# @package _global_

defaults:
  - override /data: reddit
  - override /model: dygformer
  - override /callbacks: default
  - override /trainer: gpu

logger:
  wandb:
    tags: ${tags}
    group: reddit-dygformer-separate-scale-ablation-historical

tags: ["dev"]

seed: 1

callbacks:
  early_stopping:
    monitor: val/random/ap
    mode: max
    patience: 20
  model_checkpoint:
    monitor: val/random/ap
    mode: max

trainer:
  min_epochs: 10
  max_epochs: 100
  devices: [1]
  num_sanity_val_steps: 0

model:
  max_input_sequence_length: 64
  patch_size: 2
  embed_method: separate
  channel_embedding_dim: 30
  dropout: 0.5
  scale_timediff: true

data:
  train_negative_sample_strategy: historical
  eval_negative_sample_strategy: historical
